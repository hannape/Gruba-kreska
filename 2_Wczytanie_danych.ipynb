{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Wczytanie_danych.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoJo1x1k3mnmGo4KB6r5aY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannape/Gruba-kreska/blob/main/2_Wczytanie_danych.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKFQrzjGsl8N"
      },
      "source": [
        "Będzie to funkcja która pozwoli wczytać dane z pliku wav, podzieli je na M-ms klipy (500 ms domyślne) z N-sek overlapem (150 ms), wczyta etykiety z pliku txt (jeśli więcej niż 4ms głosu w próbce - to pozytywna), stworzy wektory \n",
        "\n",
        "* X_train i Y_train\n",
        "* X_valid i Y_valid\n",
        "\n",
        "W sumie mam 94 nagrania. \n",
        "\n",
        "* treningowy - 87 nagrań, balansujemy, jeśli są dostępne inne etykiety niż interesujących nas głosów (np gh, t, g), to dodajemy je jako próbki negatywne. Jeśłi nie ma głosów w nagraniu to bierzemy P (50) próbek losowych z tego nagrania. \n",
        "* walidacyjny to 7-8 nagrań, które balansujemy tylko trochę - np z poziomu 1:100, do poziomu 1:10.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwdJB9sPsfM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563ef0f5-da75-4e43-e783-af307f936626"
      },
      "source": [
        "# Wczytywanie google drive'a\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from __future__ import print_function"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DndhmZEerl4"
      },
      "source": [
        "import os\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import wave\n",
        "import math\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b04WgWNxetjQ"
      },
      "source": [
        "# Parametry które będą wchodzić do funkcji\n",
        "\n",
        "path_test1618_txt = 'drive/My Drive/poprawione etykiety 24112020/testowe/'\n",
        "path_train161718_txt =  'drive/My Drive/poprawione etykiety 24112020/treningowe/'\n",
        "path_test1618_wav = 'drive/My Drive/testowe_1618/'\n",
        "path_train161718_wav =  'drive/My Drive/treningowe94_161718/'\n",
        "balance_type = ['balanced', 'valid', 'full_rec']  # na potrzeby treningu, valida, testu\n",
        "chunk_length_ms = 500 # 500 ms\n",
        "chunk_overlap = 150 # 150 ms\n",
        "calls_0 = ['t', 'g', 'czapla', 'gh', 'puszczyk']                                    # etykiety które są negatywne\n",
        "calls_1 = ['d', 'd?', 'k', 'k?', 'kwiczol', 'r','r?', 's', 's?', 'skowronek', 'ni'] # calls of interest\n",
        "calls_unknown = ['???','??? mysz', '??? high freq']                                 # unknown, nie wiem czy to głos czy nie, więc będę wyrzucać te chunksy to zawierające\n",
        "tolerance = 0.004  # jaka tolerancja, w sek (jeśli mniej niż ta wartość znajdzie się w chunksie, to uznajemy że głosu tam nie ma)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f5ujDUmeuLQ"
      },
      "source": [
        "def my_read_labels(label_name,path_txt):  # ta sama funkcja co w 1. poprawianie danych\n",
        "    '''Wczytuje etykiety czasowe z pliku labels.txt w folderze train.\n",
        "    \n",
        "    Returns:\n",
        "        ndarray: Tablica z etykietami czasowymi zawierająca kolumny:  sekunda początku dźwięku, sekunda końca dźwięku, gatunek.\n",
        "    '''\n",
        "    labels = []\n",
        "    with open(os.path.join(path_txt, label_name + '.txt'), 'r') as file:\n",
        "        text = file.read()\n",
        "        for line in text.split('\\n')[0:]:\n",
        "            if len(line) > 1:\n",
        "                start, stop, spec = line.split('\\t')\n",
        "                #print(start)\n",
        "                start, stop, spec = float(start), float(stop), str(spec),\n",
        "                labels.append([start, stop, spec])\n",
        "    return np.array(labels)\n",
        "\n",
        "\n",
        "def my_check_labels(second, chunk_length_s, labels, tol=0.):\n",
        "    '''Sprawdza czy w ramce czasowej [second, second + chunk_length_s] znajduje się coś wg etykiet `labels`.\n",
        "    \n",
        "    Args:\n",
        "        second (float): Sekunda nagrania.\n",
        "        labels (ndarray): Tablica z etykietami, której 1 kolumna oznacza początek, a 2ga - koniec nagrania.\n",
        "        tol (float): Tolerancja na brzegach fragmentu. Dźwięk, żeby był uznany, musi się kończyć po czasie `second+tol`\n",
        "            lub zaczynać przed czasem `second+chunk_length-tol`.\n",
        "    Returns:\n",
        "        bool: Czy w ramce czasowej jest etykieta - co najmniej 4 ms labelu.\n",
        " ''' \n",
        "    \n",
        "  #calls_to_cut = ['t', 't?', 't ','t  ', 'g', 'czapla', 'gh', 'g cz', 'puszczyk','gaski','g?','mewa?','zwierzak?','high freq','g niskie','??? mysz']\n",
        "\n",
        "  # jeśli początek etykiety zawiera się w chunku, najpóźniej 4ms przed końcem chunka\n",
        "  # jeśli koniec etykiety zawiera się w chunku, i najwcześniej 4 ms po rozpoczęciu chunka\n",
        "  # jeśli początek etykiety zaczyna się rpzed, a koniec po (w przypadku głosów dłuższych niż chunk length, b. rzadkie przypadki dla 0.5 sek)\n",
        "    return (float(labels[0]) >= second and float(labels[0]) < second + chunk_length_s - tol) or \\\n",
        "           (float(labels[1]) < second + chunk_length_s and float(labels[1]) > second + tol) or \\\n",
        "           (float(labels[0]) < second and float(labels[1]) > second + chunk_length_s)              \n",
        "           #and (labels[2] in calls_to_cut)    # nie sprawdzajmy tu jaka to etykieta jest\n",
        "\n",
        "def my_map_seconds_to_y(labels, recording_duration, calls_of_interest, calls_to_cut, calls_unknown):  ## Chcemy 500 ms z 150 ms overlapem\n",
        "    '''Tworzy etykiety dla każdego kolejnego chunksa. 1 gdy co najmniej 4 ms etykiety się znajdują w chunksie (gdy urwane dźwięki na brzegach, <4ms - to 0).\n",
        "    \n",
        "    Args:\n",
        "        labels (ndarray): Tablica z etykietami, której 1 kolumna oznacza początek, a druga - koniec nagrania.\n",
        "    Returns:\n",
        "        ndarray: Tablica z binarnymi etykietami.\n",
        "    '''\n",
        "    #STARE: calls_to_cut = ['t', 't?', 't ','t  ', 'g', 'czapla', 'gh', 'g cz', 'puszczyk','gaski','g?','mewa?','zwierzak?','high freq','g niskie','??? mysz']\n",
        "    \n",
        "    \n",
        "    duration_in_ms = recording_duration*1000\n",
        "    nr_of_chunks =  1 + (duration_in_ms - chunk_length_ms) / (chunk_length_ms - chunk_overlap)\n",
        "    print(\"ilość chunksów w tym nagraniu: \", math.ceil(nr_of_chunks) )\n",
        "    \n",
        "    \n",
        "    y = [0] * math.ceil(nr_of_chunks)             # recording_duration \n",
        "    y_restrictive = [0] * math.ceil(nr_of_chunks) # recording_duration\n",
        "    chunks_start = [0] * math.ceil(nr_of_chunks)     \n",
        "    chunks_end = [0] * math.ceil(nr_of_chunks) \n",
        "    has_unknown = [0] * math.ceil(nr_of_chunks)\n",
        "    call_id = [[] for _ in range(math.ceil(nr_of_chunks))]\n",
        "    chunks_species = [[] for _ in range(math.ceil(nr_of_chunks))]\n",
        "    \n",
        "    print(np.shape(labels))\n",
        "    for s in range(math.ceil(nr_of_chunks)):\n",
        "        chunks_start[s] = s * ((chunk_length_ms-chunk_overlap) / 1000)  # czyli s * 0.35\n",
        "        chunks_end[s] = chunks_start[s] + chunk_length_ms/1000 \n",
        "        for ind,l in enumerate(labels):\n",
        "            #print('label_index: ',ind, ' a label to: ', l[2] )\n",
        "            if my_check_labels(chunks_start[s], chunk_length_ms/1000, l):\n",
        "                #y[s] = 1\n",
        "                #print(s*0.35)\n",
        "                if l[2] in calls_to_cut:\n",
        "                  y[s] = 0\n",
        "                if l[2] in calls_unknown:\n",
        "                  y[s] = 0 \n",
        "                  has_unknown[s] = 1\n",
        "                if l[2] in calls_of_interest:  # ostatnie, bo jeśli jest więcej niż 1 etykieta, to najważniejsze jest że w chunksie jest też głos\n",
        "                  y[s] = 1 \n",
        "                    \n",
        "                  #chunks_species[s]= l[2]  \n",
        "                  #call_id[s] = ind[2] \n",
        "\n",
        "            if my_check_labels(chunks_start[s], chunk_length_ms/1000, l, 0.004): #z tolerancją \n",
        "                  #y_restrictive[s] = 0\n",
        "                  #print(\"wyciety glos:\")\n",
        "                  #print(l[2])\n",
        "                if l[2] in calls_to_cut:\n",
        "                  y_restrictive[s] = 0\n",
        "                if l[2] in calls_unknown:\n",
        "                  y_restrictive[s] = 0  \n",
        "                  has_unknown[s] = 1\n",
        "                if l[2] in calls_of_interest:  # ostatnie, bo jeśli jest więcej niż 1 etykieta, to najważniejsze jest że w chunksie jest też głos\n",
        "                  \n",
        "                  #if y_restrictive[s] == 0:  # pierwszy label \n",
        "                  y_restrictive[s] = 1  \n",
        "                  chunks_species[s].append(l[2])   # może być w tym, nie w restrictive - by było wiadomo jaki typ głosu zostanie wycięty\n",
        "                  call_id[s].append(ind)\n",
        "              \n",
        "                     \n",
        "\n",
        "\n",
        "\n",
        "                 \n",
        "        if y[s] != y_restrictive[s] and l[2] in calls_of_interest:\n",
        "            print('próbka ', s, 'zaczynajaca sie ', chunks_start[s],', jest unknown, bo za mały fragment głosu ')\n",
        "            #print('y[s]:', y[s],'yres:', y_restrictive[s] )\n",
        "            y[s] = 0    # jeśli mniej niż 4 ms fragment, to zakładamy że nie ma głosu, i dajemy unknown także, by nie karać za ewentualne znalezienie tego głosu\n",
        "            has_unknown[s] = 1\n",
        "    print('Rozmiar ',np.shape([s, chunks_start, chunks_end, y, chunks_species, call_id, has_unknown]))\n",
        "    return s+1, chunks_start, chunks_end, y, chunks_species, call_id, has_unknown    \n",
        "\n",
        "\n",
        "def my_load_wav(path_wav, path_txt):   # dłuższa funkcja niż w 1.\n",
        "\n",
        "  recording_labels_all = []\n",
        "  in_which_recording = []\n",
        "  file_names = []\n",
        "\n",
        "  rec_files = [file_name for file_name in os.listdir(path_wav) if file_name.endswith('.wav')]\n",
        "\n",
        "  print(np.size(rec_files))\n",
        "\n",
        "  X_matrix = [[] for _ in range(np.size(rec_files))]\n",
        "  file_names = [[] for _ in range(np.size(rec_files))]\n",
        "\n",
        "  for ind, file_name in enumerate(rec_files):\n",
        "      \n",
        "      print(\"------------Analiza nagrania: \" + file_name + \"-----------\")\n",
        "      recording_id = str(file_name.split('.')[0])\n",
        "      \n",
        "      fname = path_wav + file_name      #print(fname)\n",
        "      with contextlib.closing(wave.open(fname,'r')) as f:\n",
        "          frames = f.getnframes()\n",
        "          rate = f.getframerate()\n",
        "          duration = frames / float(rate)\n",
        "          recording_duration = math.ceil(duration)\n",
        "          print(\"Czas trwania nagrania - w sekundach: \" + str(recording_duration))\n",
        "          #print(\"rate:\", rate)\n",
        "    \n",
        "      recording_id = (file_name.split('.')[0])       #print(recording_id)\n",
        "      recording_labels = my_read_labels(recording_id, path_txt)    \n",
        "\n",
        "      print(\"Duration:\", duration)\n",
        "      #y_binary = my_map_seconds_to_y(recording_labels, duration)\n",
        "      chunk_id, chunk_start, chunk_end, has_bird, chunks_species, call_id, has_unknown  = my_map_seconds_to_y(recording_labels, duration, calls_1, calls_0, calls_unknown)\n",
        "      chunk_ids = range(chunk_id)\n",
        "      chunk_start = rate*np.array(chunk_start)\n",
        "      chunk_start = [round(num) for num in chunk_start]\n",
        "      chunk_end = rate*np.array(chunk_end)\n",
        "      chunk_end = [round(num) for num in chunk_end]\n",
        "      X_matrix[ind] = [chunk_ids, chunk_start, chunk_end, has_bird, chunks_species, call_id, has_unknown]  # !!!! chunk start i end  przerzucone na sample 1!!!!!\n",
        "      file_names[ind] = file_name\n",
        "\n",
        "  return X_matrix , file_names #chunk_id, chunk_start, chunk_end, has_bird, chunks_species, call_id, has_unknown"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn14hD_8LFxu",
        "outputId": "d54443f1-fcd7-4f24-e801-7c2f452ce437"
      },
      "source": [
        "#chunk_id, chunk_start, chunk_end, has_bird, chunks_species, call_id, has_unknown = my_load_wav(path_train161718_wav, path_train161718_txt) \n",
        "\n",
        "X_train, file_names = my_load_wav(path_test1618_wav, path_test1618_txt) \n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "------------Analiza nagrania: BUK4_20160922_005604.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1799\n",
            "Duration: 1798.002358276644\n",
            "ilość chunksów w tym nagraniu:  5137\n",
            "(40, 3)\n",
            "próbka  1461 zaczynajaca sie  511.34999999999997 , jest unknown, bo za mały fragment głosu \n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK4_20161016_035704.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1799\n",
            "Duration: 1798.002358276644\n",
            "ilość chunksów w tym nagraniu:  5137\n",
            "(28, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK5_20161101_002104b.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1799\n",
            "Duration: 1798.002358276644\n",
            "ilość chunksów w tym nagraniu:  5137\n",
            "(273, 3)\n",
            "próbka  436 zaczynajaca sie  152.6 , jest unknown, bo za mały fragment głosu \n",
            "próbka  861 zaczynajaca sie  301.34999999999997 , jest unknown, bo za mały fragment głosu \n",
            "próbka  972 zaczynajaca sie  340.2 , jest unknown, bo za mały fragment głosu \n",
            "próbka  1075 zaczynajaca sie  376.25 , jest unknown, bo za mały fragment głosu \n",
            "próbka  2293 zaczynajaca sie  802.55 , jest unknown, bo za mały fragment głosu \n",
            "próbka  3421 zaczynajaca sie  1197.35 , jest unknown, bo za mały fragment głosu \n",
            "próbka  4573 zaczynajaca sie  1600.55 , jest unknown, bo za mały fragment głosu \n",
            "próbka  4576 zaczynajaca sie  1601.6 , jest unknown, bo za mały fragment głosu \n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK4_20161016_012704.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1799\n",
            "Duration: 1798.002358276644\n",
            "ilość chunksów w tym nagraniu:  5137\n",
            "(6, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK4_20161008_004605.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1798\n",
            "Duration: 1797.0009977324264\n",
            "ilość chunksów w tym nagraniu:  5134\n",
            "(43, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK5_20161101_002104a.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1799\n",
            "Duration: 1798.002358276644\n",
            "ilość chunksów w tym nagraniu:  5137\n",
            "(147, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK5_20181020_014805a.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1798\n",
            "Duration: 1797.0009977324264\n",
            "ilość chunksów w tym nagraniu:  5134\n",
            "(67, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK1_20181013$023504.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1799\n",
            "Duration: 1798.002358276644\n",
            "ilość chunksów w tym nagraniu:  5137\n",
            "(93, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK5_20180921_015906b.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1797\n",
            "Duration: 1796.0025396825397\n",
            "ilość chunksów w tym nagraniu:  5132\n",
            "(55, 3)\n",
            "próbka  4285 zaczynajaca sie  1499.75 , jest unknown, bo za mały fragment głosu \n",
            "próbka  5024 zaczynajaca sie  1758.3999999999999 , jest unknown, bo za mały fragment głosu \n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK5_20181103_011804.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1799\n",
            "Duration: 1798.002358276644\n",
            "ilość chunksów w tym nagraniu:  5137\n",
            "(49, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK5_20181028_043105.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1798\n",
            "Duration: 1797.0009977324264\n",
            "ilość chunksów w tym nagraniu:  5134\n",
            "(58, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK1_20181011$001004.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1799\n",
            "Duration: 1798.002358276644\n",
            "ilość chunksów w tym nagraniu:  5137\n",
            "(169, 3)\n",
            "próbka  173 zaczynajaca sie  60.55 , jest unknown, bo za mały fragment głosu \n",
            "próbka  3272 zaczynajaca sie  1145.1999999999998 , jest unknown, bo za mały fragment głosu \n",
            "próbka  5043 zaczynajaca sie  1765.05 , jest unknown, bo za mały fragment głosu \n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK4_20180925_224703.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1800\n",
            "Duration: 1799.0008163265306\n",
            "ilość chunksów w tym nagraniu:  5140\n",
            "(26, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK5_20180909_010005.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1798\n",
            "Duration: 1797.0009977324264\n",
            "ilość chunksów w tym nagraniu:  5134\n",
            "(16, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK1_20180918$040704.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1799\n",
            "Duration: 1798.002358276644\n",
            "ilość chunksów w tym nagraniu:  5137\n",
            "(56, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK5_20180921_015906a.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1797\n",
            "Duration: 1796.0025396825397\n",
            "ilość chunksów w tym nagraniu:  5132\n",
            "(57, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK5_20181010_011205.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1798\n",
            "Duration: 1797.0009977324264\n",
            "ilość chunksów w tym nagraniu:  5134\n",
            "(41, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK4_20180913_201703.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1800\n",
            "Duration: 1799.0008163265306\n",
            "ilość chunksów w tym nagraniu:  5140\n",
            "(23, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK5_20181014_233005.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1798\n",
            "Duration: 1797.0009977324264\n",
            "ilość chunksów w tym nagraniu:  5134\n",
            "(20, 3)\n",
            "Rozmiar  (7,)\n",
            "------------Analiza nagrania: BUK5_20181021_004605deszczyk.wav-----------\n",
            "Czas trwania nagrania - w sekundach: 1798\n",
            "Duration: 1797.0009977324264\n",
            "ilość chunksów w tym nagraniu:  5134\n",
            "(25, 3)\n",
            "Rozmiar  (7,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzxwT3r9SvLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb0a4b4-fa57-49bc-e623-30b8bc529e44"
      },
      "source": [
        "print(np.shape(X_train))\n",
        "czas_trwania_spr= np.array(X_train[0][1])- np.array(X_train[0][2])\n",
        "print(np.unique(czas_trwania_spr)) # upewnienie się że wszystkie chunki jednakowej długości są\n",
        "print(np.shape(file_names))\n",
        "\n",
        "result_dataframe = pd.DataFrame(data = X_train, index=file_names, columns = ['chunk_ids', 'chunk_start', 'chunk_end', 'has_bird', 'chunks_species', 'call_id', 'has_unknown'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20, 7)\n",
            "[-22050.]\n",
            "(20,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "heidXnnni4QQ"
      },
      "source": [
        "#@title\n",
        "a = (X_train[17][1])\n",
        "print(type(X_train))\n",
        "print(type(result_dataframe['chunk_start']))\n",
        "print(result_dataframe['chunk_start'][\"BUK4_20160922_005604.wav\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F96of4-ewgIF",
        "outputId": "b71ee831-ee39-4cad-ce3e-045d394a669f"
      },
      "source": [
        "# do sprawdzenia czy te zagnieżdżone listy w call_id i chunks_species działają\n",
        "\n",
        "def isListEmpty(inList):\n",
        "  for elem in inList:\n",
        "    if elem: # Is a list\n",
        "        print(elem)\n",
        "        #return all( map(isListEmpty, inList) )\n",
        "    #return False # Not a list\n",
        "\n",
        "isListEmpty(result_dataframe['call_id'][2])\n",
        "isListEmpty(result_dataframe['chunks_species'][2])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4]\n",
            "[4]\n",
            "[5]\n",
            "[5]\n",
            "[6, 7]\n",
            "[6, 7, 8, 9]\n",
            "[8, 9]\n",
            "[11]\n",
            "[11]\n",
            "[12]\n",
            "[16]\n",
            "[16, 17]\n",
            "[17]\n",
            "[19]\n",
            "[19, 20]\n",
            "[20]\n",
            "[21]\n",
            "[22]\n",
            "[22]\n",
            "[23]\n",
            "[24]\n",
            "[24]\n",
            "[25]\n",
            "[25]\n",
            "[26]\n",
            "[26]\n",
            "[27]\n",
            "[27, 28]\n",
            "[28]\n",
            "[29, 30]\n",
            "[30]\n",
            "[31]\n",
            "[31, 32]\n",
            "[33]\n",
            "[34]\n",
            "[34]\n",
            "[35]\n",
            "[36]\n",
            "[36]\n",
            "[37]\n",
            "[37, 38]\n",
            "[38]\n",
            "[39]\n",
            "[39, 40]\n",
            "[40]\n",
            "[41]\n",
            "[42, 43]\n",
            "[42, 43]\n",
            "[44]\n",
            "[44, 45]\n",
            "[45, 46]\n",
            "[46]\n",
            "[48]\n",
            "[48]\n",
            "[48, 49]\n",
            "[49]\n",
            "[50]\n",
            "[50]\n",
            "[51]\n",
            "[52]\n",
            "[52]\n",
            "[53]\n",
            "[53, 54]\n",
            "[54, 55]\n",
            "[55, 56]\n",
            "[57]\n",
            "[57]\n",
            "[58]\n",
            "[59]\n",
            "[61]\n",
            "[61]\n",
            "[62]\n",
            "[62, 63]\n",
            "[63]\n",
            "[64]\n",
            "[64, 65, 66]\n",
            "[65, 66]\n",
            "[66]\n",
            "[67]\n",
            "[67]\n",
            "[68]\n",
            "[68]\n",
            "[69]\n",
            "[70]\n",
            "[70]\n",
            "[70, 71]\n",
            "[71]\n",
            "[71]\n",
            "[71]\n",
            "[72]\n",
            "[72]\n",
            "[73]\n",
            "[73, 74]\n",
            "[74]\n",
            "[75]\n",
            "[75]\n",
            "[75]\n",
            "[76]\n",
            "[76, 77]\n",
            "[77]\n",
            "[78]\n",
            "[79]\n",
            "[79]\n",
            "[80]\n",
            "[80]\n",
            "[81]\n",
            "[81]\n",
            "[82]\n",
            "[82]\n",
            "[83]\n",
            "[83]\n",
            "[84]\n",
            "[84]\n",
            "[84]\n",
            "[85]\n",
            "[85]\n",
            "[86]\n",
            "[86, 87]\n",
            "[87]\n",
            "[88]\n",
            "[88, 89]\n",
            "[89]\n",
            "[92]\n",
            "[94]\n",
            "[94]\n",
            "[95, 96]\n",
            "[96, 97]\n",
            "[97]\n",
            "[98]\n",
            "[98]\n",
            "[99]\n",
            "[99, 100, 101]\n",
            "[101]\n",
            "[102]\n",
            "[102]\n",
            "[103]\n",
            "[103, 104]\n",
            "[104]\n",
            "[105]\n",
            "[105]\n",
            "[106]\n",
            "[106, 107]\n",
            "[107, 108]\n",
            "[108]\n",
            "[109]\n",
            "[109, 110]\n",
            "[110, 111]\n",
            "[111]\n",
            "[112]\n",
            "[112]\n",
            "[113]\n",
            "[113, 114]\n",
            "[114]\n",
            "[115]\n",
            "[115]\n",
            "[116]\n",
            "[116]\n",
            "[117]\n",
            "[117]\n",
            "[118]\n",
            "[118]\n",
            "[119]\n",
            "[120]\n",
            "[120]\n",
            "[121]\n",
            "[121, 122]\n",
            "[122]\n",
            "[123]\n",
            "[123]\n",
            "[124]\n",
            "[124, 125]\n",
            "[125]\n",
            "[126]\n",
            "[126]\n",
            "[127]\n",
            "[127, 128]\n",
            "[128, 129]\n",
            "[129]\n",
            "[130]\n",
            "[130]\n",
            "[131]\n",
            "[132]\n",
            "[132]\n",
            "[133]\n",
            "[133]\n",
            "[134]\n",
            "[134, 135]\n",
            "[136]\n",
            "[136]\n",
            "[137]\n",
            "[137, 138]\n",
            "[138]\n",
            "[139]\n",
            "[140]\n",
            "[140]\n",
            "[141]\n",
            "[141]\n",
            "[142]\n",
            "[143]\n",
            "[143, 144]\n",
            "[144]\n",
            "[145]\n",
            "[146]\n",
            "[146]\n",
            "[147]\n",
            "[147]\n",
            "[148]\n",
            "[148]\n",
            "[149]\n",
            "[151]\n",
            "[151]\n",
            "[152]\n",
            "[152, 153]\n",
            "[153]\n",
            "[154]\n",
            "[154]\n",
            "[155]\n",
            "[155]\n",
            "[156]\n",
            "[156, 157]\n",
            "[157]\n",
            "[158]\n",
            "[158]\n",
            "[158]\n",
            "[160]\n",
            "[160]\n",
            "[161]\n",
            "[161]\n",
            "[162]\n",
            "[162]\n",
            "[163]\n",
            "[163]\n",
            "[164]\n",
            "[164]\n",
            "[165]\n",
            "[165]\n",
            "[166]\n",
            "[166]\n",
            "[166]\n",
            "[167]\n",
            "[167]\n",
            "[168, 169]\n",
            "[168, 169]\n",
            "[170]\n",
            "[170]\n",
            "[171]\n",
            "[171]\n",
            "[172]\n",
            "[172, 173]\n",
            "[173]\n",
            "[174]\n",
            "[174, 175]\n",
            "[175]\n",
            "[176]\n",
            "[177]\n",
            "[177, 178]\n",
            "[178]\n",
            "[179]\n",
            "[179, 180]\n",
            "[179, 180]\n",
            "[180]\n",
            "[181, 182]\n",
            "[182]\n",
            "[183]\n",
            "[183, 184]\n",
            "[184]\n",
            "[185]\n",
            "[185, 186]\n",
            "[186]\n",
            "[187]\n",
            "[187]\n",
            "[188]\n",
            "[188]\n",
            "[189]\n",
            "[189, 190]\n",
            "[190]\n",
            "[191]\n",
            "[191]\n",
            "[192]\n",
            "[192, 193]\n",
            "[193]\n",
            "[194]\n",
            "[194]\n",
            "[195]\n",
            "[195]\n",
            "[196]\n",
            "[196, 197]\n",
            "[197]\n",
            "[200]\n",
            "[200, 201]\n",
            "[201]\n",
            "[202]\n",
            "[202, 203]\n",
            "[203]\n",
            "[204]\n",
            "[204]\n",
            "[205]\n",
            "[205]\n",
            "[206]\n",
            "[206]\n",
            "[207]\n",
            "[207, 208, 209]\n",
            "[208, 209]\n",
            "[210]\n",
            "[210]\n",
            "[211]\n",
            "[211]\n",
            "[212]\n",
            "[212, 213]\n",
            "[213]\n",
            "[214]\n",
            "[214, 215, 216]\n",
            "[215, 216]\n",
            "[216]\n",
            "[217]\n",
            "[217]\n",
            "[218]\n",
            "[218]\n",
            "[219]\n",
            "[219]\n",
            "[221]\n",
            "[221]\n",
            "[222]\n",
            "[222]\n",
            "[223]\n",
            "[223]\n",
            "[224]\n",
            "[224, 225]\n",
            "[226]\n",
            "[226]\n",
            "[227]\n",
            "[227]\n",
            "[228]\n",
            "[228, 229]\n",
            "[229]\n",
            "[230]\n",
            "[230, 231]\n",
            "[231]\n",
            "[232]\n",
            "[232]\n",
            "[233]\n",
            "[233]\n",
            "[234]\n",
            "[234]\n",
            "[235]\n",
            "[235]\n",
            "[236]\n",
            "[236, 237]\n",
            "[237]\n",
            "[238]\n",
            "[239]\n",
            "[239, 240]\n",
            "[240]\n",
            "[241]\n",
            "[241]\n",
            "[242]\n",
            "[242]\n",
            "[243]\n",
            "[243]\n",
            "[244]\n",
            "[244]\n",
            "[245]\n",
            "[245, 246]\n",
            "[246, 247]\n",
            "[247]\n",
            "[248]\n",
            "[248]\n",
            "[249]\n",
            "[250, 251]\n",
            "[250, 251, 252]\n",
            "[252]\n",
            "[253]\n",
            "[254]\n",
            "[254, 255]\n",
            "[255]\n",
            "[256]\n",
            "[256]\n",
            "[257]\n",
            "[258]\n",
            "[258]\n",
            "[259, 260]\n",
            "[260]\n",
            "[261]\n",
            "[261, 262]\n",
            "[262]\n",
            "[263]\n",
            "[263]\n",
            "[265]\n",
            "[266]\n",
            "[266]\n",
            "[267]\n",
            "[267]\n",
            "[268]\n",
            "[268]\n",
            "[269]\n",
            "[269]\n",
            "[270]\n",
            "[270, 271]\n",
            "[271]\n",
            "[272]\n",
            "[272]\n",
            "['s?']\n",
            "['s?']\n",
            "['k']\n",
            "['k']\n",
            "['ni', 'ni']\n",
            "['ni', 'ni', 'ni', 'ni']\n",
            "['ni', 'ni']\n",
            "['k']\n",
            "['k']\n",
            "['ni']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['s']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['s?']\n",
            "['s?']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['ni']\n",
            "['k']\n",
            "['k']\n",
            "['s']\n",
            "['s']\n",
            "['s']\n",
            "['k']\n",
            "['k', 'd?']\n",
            "['d?']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k?']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k?']\n",
            "['k?']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k?']\n",
            "['d']\n",
            "['d']\n",
            "['k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['d']\n",
            "['d']\n",
            "['k']\n",
            "['k', 'k', 'k']\n",
            "['k']\n",
            "['ni']\n",
            "['ni']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['d']\n",
            "['d']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['d']\n",
            "['d']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['d']\n",
            "['d']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['d']\n",
            "['d']\n",
            "['d']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['ni']\n",
            "['ni']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['ni']\n",
            "['ni']\n",
            "['k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k', 'k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k?']\n",
            "['k?']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['s']\n",
            "['s']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k', 'ni']\n",
            "['ni']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k', 'k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k?', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k?']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n",
            "['k', 'k']\n",
            "['k']\n",
            "['k']\n",
            "['k']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h047_V8wnno",
        "outputId": "d677ea16-ed94-492b-b5fb-b90d9f553741"
      },
      "source": [
        "# do sprawdzenia jakie ID mają głosy w naszym nagraniu (każdy z labeli ma ID, nie tylko głosy, dlatego nie wszystkie liczby tu będą)\n",
        "\n",
        "test_list = result_dataframe['call_id'][2]\n",
        "\n",
        "#print(\"The original list : \" + str(test_list)) \n",
        "  \n",
        "res = [] \n",
        "temp = set() \n",
        "for inner in test_list: \n",
        "        for ele in inner: \n",
        "            if not ele in temp: \n",
        "                temp.add(ele) \n",
        "                res.append(ele) \n",
        "  \n",
        "print(\"Unique elements in nested tuples are : \" + str(res)) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique elements in nested tuples are : [4, 5, 6, 7, 8, 9, 11, 12, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 271, 272]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1Q7SczC0e7n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}